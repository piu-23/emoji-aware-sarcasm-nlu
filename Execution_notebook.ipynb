{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1ehYc3g5XWoWE2D7XvUsfBmAZ4UE3dbF8",
      "authorship_tag": "ABX9TyMrRdMHPO2Gir4wcwCmyhhh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/piu-23/emoji-aware-sarcasm-nlu/blob/main/Execution_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DF9Cn5XRqbgf",
        "outputId": "fdd94c7b-54c9-41dd-91a8-5eaf6afddb73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'emoji-aware-sarcasm-nlu'...\n",
            "remote: Enumerating objects: 111, done.\u001b[K\n",
            "remote: Counting objects: 100% (111/111), done.\u001b[K\n",
            "remote: Compressing objects: 100% (99/99), done.\u001b[K\n",
            "Receiving objects: 100% (111/111), 46.27 KiB | 5.78 MiB/s, done.\n",
            "Resolving deltas: 100% (38/38), done.\n",
            "remote: Total 111 (delta 38), reused 16 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "/content/emoji-aware-sarcasm-nlu\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/piu-23/emoji-aware-sarcasm-nlu.git\n",
        "%cd emoji-aware-sarcasm-nlu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pandas scikit-learn emoji\n",
        "!pip -q install torch transformers accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYUmet0Xq0Oy",
        "outputId": "f9e2c204-c4cd-4263-966b-e0afa05338ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/608.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m604.2/608.4 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRXtO-9_u27N",
        "outputId": "c5339db8-e8d0-4183-fa81-e1bfed4dfe65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/emoji-aware-sarcasm-nlu\n",
        "!mkdir -p /content/emoji-aware-sarcasm-nlu\n",
        "!unzip -q \"/content/drive/MyDrive/Colab Notebooks/emoji-aware-sarcasm-nlu-main.zip\" -d /content/emoji-aware-sarcasm-nlu\n",
        "!ls -la /content/emoji-aware-sarcasm-nlu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6i6LBVXu6t5",
        "outputId": "3cf6ace4-7e43-42b0-c7b5-25fead3fe863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 16\n",
            "drwxr-xr-x 4 root root 4096 Feb 15 14:37 .\n",
            "drwxr-xr-x 1 root root 4096 Feb 15 14:37 ..\n",
            "drwxrwxr-x 7 root root 4096 Feb 15 14:34 emoji-aware-sarcasm-nlu-main\n",
            "drwxr-xr-x 3 root root 4096 Feb 15 14:37 __MACOSX\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/emoji-aware-sarcasm-nlu -maxdepth 2 -type d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RHQa27WvCD9",
        "outputId": "cc251996-6bae-4214-af9c-65cd7ab718cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/emoji-aware-sarcasm-nlu\n",
            "/content/emoji-aware-sarcasm-nlu/__MACOSX\n",
            "/content/emoji-aware-sarcasm-nlu/__MACOSX/emoji-aware-sarcasm-nlu-main\n",
            "/content/emoji-aware-sarcasm-nlu/emoji-aware-sarcasm-nlu-main\n",
            "/content/emoji-aware-sarcasm-nlu/emoji-aware-sarcasm-nlu-main/data\n",
            "/content/emoji-aware-sarcasm-nlu/emoji-aware-sarcasm-nlu-main/src\n",
            "/content/emoji-aware-sarcasm-nlu/emoji-aware-sarcasm-nlu-main/results\n",
            "/content/emoji-aware-sarcasm-nlu/emoji-aware-sarcasm-nlu-main/configs\n",
            "/content/emoji-aware-sarcasm-nlu/emoji-aware-sarcasm-nlu-main/contrib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/emoji-aware-sarcasm-nlu/emoji-aware-sarcasm-nlu-main\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_MvGYVyvGD8",
        "outputId": "46b93153-79fc-4b71-8404-0b9b21b2f014"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/emoji-aware-sarcasm-nlu/emoji-aware-sarcasm-nlu-main\n",
            "configs  contrib  data\tREADME.md  results  src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pandas scikit-learn\n",
        "!pip -q install torch transformers accelerate\n",
        "!pip -q install emoji"
      ],
      "metadata": {
        "id": "59lrtSdEvI34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UeUtztsvL7d",
        "outputId": "96397330-093a-4ef0-c703-01a7b23dd6b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 528K\n",
            "-rw-rw-r-- 1 root root 2.0K Feb 11 09:18 prepare_splits.py\n",
            "-rw-rw-r-- 1 root root  446 Feb 11 09:18 README_data.md\n",
            "-rw-rw-r-- 1 root root  33K Feb 11 09:18 splits.json\n",
            "-rw-rw-r-- 1 root root  610 Feb 11 09:18 stats.md\n",
            "-rw-r--r-- 1 root root 480K Feb 15 14:31 train.En.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python data/prepare_splits.py\n",
        "!ls -lh data/splits.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0t8SXjCvPIr",
        "outputId": "d21d745c-a41b-4bee-8a47-a7a4c22101b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "splits.json written successfully\n",
            "-rw-rw-r-- 1 root root 33K Feb 15 14:38 data/splits.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find src -maxdepth 3 -type f -name \"*tfidf*\" -o -name \"*baseline*\" -o -name \"*lr*.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbryEhKBvTRx",
        "outputId": "29b36acf-125c-49af-ffcc-b732dc8ed499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src/baselines\n",
            "src/baselines/train_tfidf_lr.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m src.baselines.train_tfidf_lr --variant text --run_name lr_tfidf_text\n",
        "!python -m src.baselines.train_tfidf_lr --variant demojized --run_name lr_tfidf_demojized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgVZSRGYvWj7",
        "outputId": "56a9b656-bede-46bd-d111-37ce899fa21e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/emoji-aware-sarcasm-nlu/emoji-aware-sarcasm-nlu-main/src/baselines/train_tfidf_lr.py\", line 10, in <module>\n",
            "    from src.experiments.common import get_variant_column, load_splits\n",
            "ModuleNotFoundError: No module named 'src.experiments'\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/emoji-aware-sarcasm-nlu/emoji-aware-sarcasm-nlu-main/src/baselines/train_tfidf_lr.py\", line 10, in <module>\n",
            "    from src.experiments.common import get_variant_column, load_splits\n",
            "ModuleNotFoundError: No module named 'src.experiments'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find src -maxdepth 2 -type d\n",
        "!ls -la src\n",
        "!ls -la src/experiments"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hto08frpvkAW",
        "outputId": "980d2572-d6de-46f2-b9a6-ea1b57933fbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src\n",
            "src/data\n",
            "src/train\n",
            "src/baselines\n",
            "src/baselines/__pycache__\n",
            "src/eval\n",
            "src/transformers\n",
            "src/experiements\n",
            "src/models\n",
            "total 44\n",
            "drwxrwxr-x 9 root root 4096 Feb 15 14:34 .\n",
            "drwxrwxr-x 7 root root 4096 Feb 15 14:34 ..\n",
            "drwxrwxr-x 3 root root 4096 Feb 15 14:38 baselines\n",
            "drwxrwxr-x 2 root root 4096 Feb 15 14:34 data\n",
            "-rw-r--r-- 1 root root 6148 Feb 15 14:34 .DS_Store\n",
            "drwxrwxr-x 2 root root 4096 Feb 11 09:18 eval\n",
            "drwxrwxr-x 2 root root 4096 Feb 11 09:18 experiements\n",
            "drwxrwxr-x 2 root root 4096 Feb 11 09:18 models\n",
            "drwxrwxr-x 2 root root 4096 Feb 11 09:18 train\n",
            "drwxrwxr-x 2 root root 4096 Feb 11 09:18 transformers\n",
            "ls: cannot access 'src/experiments': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/emoji-aware-sarcasm-nlu/emoji-aware-sarcasm-nlu-main\n",
        "!mv src/experiements src/experiments\n",
        "!touch src/__init__.py src/experiments/__init__.py\n",
        "!ls -la src"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOOlLbYivy8W",
        "outputId": "54c24447-4259-455f-9f50-bb7388a5e1af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/emoji-aware-sarcasm-nlu/emoji-aware-sarcasm-nlu-main\n",
            "total 44\n",
            "drwxrwxr-x 9 root root 4096 Feb 15 14:40 .\n",
            "drwxrwxr-x 7 root root 4096 Feb 15 14:34 ..\n",
            "drwxrwxr-x 3 root root 4096 Feb 15 14:38 baselines\n",
            "drwxrwxr-x 2 root root 4096 Feb 15 14:34 data\n",
            "-rw-r--r-- 1 root root 6148 Feb 15 14:34 .DS_Store\n",
            "drwxrwxr-x 2 root root 4096 Feb 11 09:18 eval\n",
            "drwxrwxr-x 2 root root 4096 Feb 15 14:40 experiments\n",
            "-rw-r--r-- 1 root root    0 Feb 15 14:40 __init__.py\n",
            "drwxrwxr-x 2 root root 4096 Feb 11 09:18 models\n",
            "drwxrwxr-x 2 root root 4096 Feb 11 09:18 train\n",
            "drwxrwxr-x 2 root root 4096 Feb 11 09:18 transformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m src.baselines.train_tfidf_lr --variant text --run_name lr_tfidf_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsPIZwPcv1oe",
        "outputId": "ddc9a717-7e08-439c-ad1f-d733a86cf374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[lr_tfidf_text] dev macro_f1=0.5672 acc=0.6967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m src.baselines.train_tfidf_lr --variant demojized --run_name lr_tfidf_demojized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jWnFycPv9U7",
        "outputId": "8912d2b6-450f-4325-9190-1c767ea9518a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[lr_tfidf_demojized] dev macro_f1=0.5672 acc=0.6967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la results/lr_tfidf_text\n",
        "!ls -la results/lr_tfidf_demojized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v20-VBrtwCk2",
        "outputId": "b587188c-9d7e-4068-a498-f87e5daac938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 36\n",
            "drwxr-xr-x 2 root root  4096 Feb 15 14:40 .\n",
            "drwxrwxr-x 4 root root  4096 Feb 15 14:41 ..\n",
            "-rw-r--r-- 1 root root   111 Feb 15 14:40 config.json\n",
            "-rw-r--r-- 1 root root   117 Feb 15 14:40 confusion.json\n",
            "-rw-r--r-- 1 root root   373 Feb 15 14:40 metrics.json\n",
            "-rw-r--r-- 1 root root 14607 Feb 15 14:40 preds.csv\n",
            "total 36\n",
            "drwxr-xr-x 2 root root  4096 Feb 15 14:41 .\n",
            "drwxrwxr-x 4 root root  4096 Feb 15 14:41 ..\n",
            "-rw-r--r-- 1 root root   116 Feb 15 14:41 config.json\n",
            "-rw-r--r-- 1 root root   117 Feb 15 14:41 confusion.json\n",
            "-rw-r--r-- 1 root root   373 Feb 15 14:41 metrics.json\n",
            "-rw-r--r-- 1 root root 14607 Feb 15 14:41 preds.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "for run in [\"lr_tfidf_text\", \"lr_tfidf_demojized\"]:\n",
        "    with open(f\"results/{run}/metrics.json\") as f:\n",
        "        m = json.load(f)\n",
        "    print(run, \"macro_f1=\", round(m[\"macro_f1\"], 4), \"acc=\", round(m[\"accuracy\"], 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uz5lUW1WwOfF",
        "outputId": "281138f8-9fc6-4fd6-833d-c971d4f884b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr_tfidf_text macro_f1= 0.5672 acc= 0.6967\n",
            "lr_tfidf_demojized macro_f1= 0.5672 acc= 0.6967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"data/train.En.csv\")\n",
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEiiyfEzwlQZ",
        "outputId": "915f49e4-6dc3-42c4-ba2a-fb413dad0671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Unnamed: 0', 'tweet', 'sarcastic', 'rephrase', 'sarcasm', 'irony',\n",
            "       'satire', 'understatement', 'overstatement', 'rhetorical_question'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/data/make_variants.py\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import emoji\n",
        "\n",
        "\n",
        "def is_emoji(ch: str) -> bool:\n",
        "    return ch in emoji.EMOJI_DATA\n",
        "\n",
        "\n",
        "def extract_emoji_only(text: str) -> str:\n",
        "    return \"\".join([ch for ch in text if is_emoji(ch)]).strip()\n",
        "\n",
        "\n",
        "def remove_emojis(text: str) -> str:\n",
        "    return \"\".join([ch for ch in text if not is_emoji(ch)]).strip()\n",
        "\n",
        "\n",
        "def demojize(text: str) -> str:\n",
        "    return emoji.demojize(text, delimiters=(\" \", \" \")).strip()\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--in_csv\", default=\"data/train.En.csv\")\n",
        "    ap.add_argument(\"--out_csv\", default=\"data/train.En.processed.csv\")\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    df = pd.read_csv(args.in_csv)\n",
        "    tweets = df[\"tweet\"].astype(str)\n",
        "\n",
        "    df[\"emoji_only\"] = tweets.map(extract_emoji_only)\n",
        "    df[\"text_only\"] = tweets.map(remove_emojis)\n",
        "    df[\"emoji_to_text\"] = tweets.map(demojize)\n",
        "\n",
        "    Path(args.out_csv).parent.mkdir(parents=True, exist_ok=True)\n",
        "    df.to_csv(args.out_csv, index=False)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVpmPc2yxE09",
        "outputId": "9eefefff-b9db-4203-84bc-4614be5d3a1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting src/data/make_variants.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install emoji\n",
        "!python src/data/make_variants.py --in_csv data/train.En.csv --out_csv data/train.En.processed.csv\n",
        "!ls -lh data/train.En.processed.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGJY_iDBxPmB",
        "outputId": "12b68806-f5af-442f-ad0e-4d7d50392b07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 1.3M Feb 15 14:47 data/train.En.processed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"data/train.En.processed.csv\")\n",
        "print(df.columns.tolist())\n",
        "print(df[[\"tweet\",\"text_only\",\"emoji_to_text\",\"emoji_only\"]].head(3).to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHyPohNPxTEK",
        "outputId": "bd3185fb-6d0d-4588-e849-a15bc2f37685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Unnamed: 0', 'tweet', 'sarcastic', 'rephrase', 'sarcasm', 'irony', 'satire', 'understatement', 'overstatement', 'rhetorical_question', 'emoji_only', 'text_only', 'emoji_to_text']\n",
            "                                                                                                                                                                                                                                       tweet                                                                                                                                                                                                                                text_only                                                                                                                                                                                                                                                                                            emoji_to_text emoji_only\n",
            "                                                                                                                                                                                   The only thing I got from college is a caffeine addiction                                                                                                                                                                                The only thing I got from college is a caffeine addiction                                                                                                                                                                                                                                                The only thing I got from college is a caffeine addiction        NaN\n",
            "                                                                                                       I love it when professors draw a big question mark next to my answer on an exam because I‚Äôm always like yeah I don‚Äôt either ¬Ø\\_(„ÉÑ)_/¬Ø                                                                                                    I love it when professors draw a big question mark next to my answer on an exam because I‚Äôm always like yeah I don‚Äôt either ¬Ø\\_(„ÉÑ)_/¬Ø                                                                                                                                                                    I love it when professors draw a big question mark next to my answer on an exam because I‚Äôm always like yeah I don‚Äôt either ¬Ø\\_(„ÉÑ)_/¬Ø        NaN\n",
            "Remember the hundred emails from companies when Covid started getting real? I‚Äôve gotten three in regards to support for protests. And only @SavageXFenty shared helpful links and actually said black lives matter... we love capitalism ü•∞üôåüèº Remember the hundred emails from companies when Covid started getting real? I‚Äôve gotten three in regards to support for protests. And only @SavageXFenty shared helpful links and actually said black lives matter... we love capitalism Remember the hundred emails from companies when Covid started getting real? I‚Äôve gotten three in regards to support for protests. And only @SavageXFenty shared helpful links and actually said black lives matter... we love capitalism  smiling_face_with_hearts  raising_hands_medium-light_skin_tone        ü•∞üôåüèº\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "p = Path(\"src/baselines/train_tfidf_lr.py\")\n",
        "s = p.read_text(encoding=\"utf-8\").splitlines()\n",
        "out = []\n",
        "for line in s:\n",
        "    out.append(line)\n",
        "    if line.strip() == 'p.add_argument(\"--data_dir\", default=\"data\")':\n",
        "        out.append('    p.add_argument(\"--train_file\", default=\"train.En.csv\")')\n",
        "p.write_text(\"\\n\".join(out), encoding=\"utf-8\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvlOTL9ZxnhR",
        "outputId": "0a01b360-8677-42cd-a438-450968a4a8ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2820"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"src/baselines/train_tfidf_lr.py\")\n",
        "txt = p.read_text(encoding=\"utf-8\")\n",
        "\n",
        "old = \"splits = load_splits(data_dir=args.data_dir)\"\n",
        "new = \"splits = load_splits(data_dir=args.data_dir, train_file=args.train_file)\"\n",
        "\n",
        "if old not in txt:\n",
        "    raise ValueError(\"Target line not found. Open the file and check the exact text.\")\n",
        "\n",
        "p.write_text(txt.replace(old, new), encoding=\"utf-8\")\n",
        "print(\"Patched load_splits() call to include train_file.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pBrIUV_yUbw",
        "outputId": "fa03639a-2fd1-4967-d45d-ddc7da530195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patched load_splits() call to include train_file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/splits = load_splits(data_dir=args.data_dir)/splits = load_splits(data_dir=args.data_dir, train_file=args.train_file)/' src/baselines/train_tfidf_lr.py\n",
        "!grep -n \"load_splits\" src/baselines/train_tfidf_lr.py | head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bB04XSugya9e",
        "outputId": "2488a79e-f699-4b58-ebb1-ce3c6d8fd3ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10:from src.experiments.common import get_variant_column, load_splits\n",
            "30:    splits = load_splits(data_dir=args.data_dir, train_file=args.train_file)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"src/baselines/train_tfidf_lr.py\")\n",
        "lines = p.read_text(encoding=\"utf-8\").splitlines()\n",
        "\n",
        "out = []\n",
        "seen = 0\n",
        "for line in lines:\n",
        "    if 'add_argument(\"--train_file\"' in line:\n",
        "        seen += 1\n",
        "        if seen >= 2:\n",
        "            continue\n",
        "    out.append(line)\n",
        "\n",
        "p.write_text(\"\\n\".join(out) + \"\\n\", encoding=\"utf-8\")\n",
        "print(\"Removed duplicate --train_file (kept the first one).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2suKfyhcyqBh",
        "outputId": "3a318620-8acf-4ba1-8321-1a9e4f4f07da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed duplicate --train_file (kept the first one).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -n \"load_splits\" src/baselines/train_tfidf_lr.py | head -n 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDhSPy9wyuyN",
        "outputId": "d39e3f6e-f0c2-43c0-fcb8-4497ed0d2593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10:from src.experiments.common import get_variant_column, load_splits\n",
            "29:    splits = load_splits(data_dir=args.data_dir, train_file=args.train_file)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m src.baselines.train_tfidf_lr --variant text --run_name lr_tfidf_text_proc --train_file train.En.processed.csv\n",
        "!python -m src.baselines.train_tfidf_lr --variant demojized --run_name lr_tfidf_demojized_proc --train_file train.En.processed.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Or4RlH8gyzWZ",
        "outputId": "b94fbdbe-0836-4021-a618-0cd5a0540f0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[lr_tfidf_text_proc] dev macro_f1=0.5672 acc=0.6967\n",
            "[lr_tfidf_demojized_proc] dev macro_f1=0.5687 acc=0.6987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find src/transformers -maxdepth 2 -type f -name \"*.py\" -print\n",
        "!ls -la src/transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5geIMysy8Mz",
        "outputId": "bf442b4a-1665-4833-bcb0-33064aca41eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src/transformers/hf_dataset.py\n",
            "src/transformers/train_transformer.py\n",
            "total 20\n",
            "drwxrwxr-x  2 root root 4096 Feb 11 09:18 .\n",
            "drwxrwxr-x 10 root root 4096 Feb 15 14:40 ..\n",
            "-rw-rw-r--  1 root root  850 Feb 11 09:18 hf_dataset.py\n",
            "-rw-rw-r--  1 root root 5091 Feb 11 09:18 train_transformer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la configs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwaMcR3Iy-oi",
        "outputId": "7ee3334e-61ca-4185-8021-abb6cda24c33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 16\n",
            "drwxrwxr-x 2 root root 4096 Feb 11 09:18 .\n",
            "drwxrwxr-x 7 root root 4096 Feb 15 14:34 ..\n",
            "-rw-rw-r-- 1 root root  112 Feb 11 09:18 gated_fusion.yaml\n",
            "-rw-rw-r-- 1 root root    0 Feb 11 09:18 .gitkeep.txt\n",
            "-rw-rw-r-- 1 root root  135 Feb 11 09:18 transformer_text_only.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find src/train -maxdepth 2 -type f -name \"*.py\" -print\n",
        "!find src/transformers -maxdepth 2 -type f -name \"*.py\" -print"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHkjM5hmzGAb",
        "outputId": "d9b182a6-8fb7-4fe8-cf89-5a3c5e170a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src/train/train_gated_fusion.py\n",
            "src/train/train_transformer.py\n",
            "src/train/__init__.py\n",
            "src/transformers/hf_dataset.py\n",
            "src/transformers/train_transformer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -R \"yaml\" -n src/train src/transformers | head -n 40"
      ],
      "metadata": {
        "id": "iUmwzhYKzIYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find . -name \"transformer_text_only.yaml\" -o -name \"gated_fusion.yaml\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYtpXuK41aKW",
        "outputId": "6a3c32ac-6b64-4610-a977-a6e35ae3a492"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./configs/gated_fusion.yaml\n",
            "./configs/transformer_text_only.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat configs/transformer_text_only.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjwPlAL-1kFb",
        "outputId": "0c5c08af-ccda-477d-90d4-85663df1fc57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_type: transformer\n",
            "input_variant: text_only\n",
            "pretrained_name: roberta-base\n",
            "max_len: 128\n",
            "batch_size: 16\n",
            "lr: 2e-5\n",
            "epochs: 4\n",
            "seed: 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -R \"model_type\" -n src | head -n 40"
      ],
      "metadata": {
        "id": "TnxA8Sag1qHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find src -maxdepth 2 -type f -name \"*.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxjueflF1suS",
        "outputId": "75bd1ebe-47c2-4508-846c-944d003582ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src/data/preprocess.py\n",
            "src/data/loader.py\n",
            "src/data/__init__.py\n",
            "src/data/make_variants.py\n",
            "src/train/train_gated_fusion.py\n",
            "src/train/train_transformer.py\n",
            "src/train/__init__.py\n",
            "src/experiments/metrics.py\n",
            "src/experiments/common.py\n",
            "src/experiments/io.py\n",
            "src/experiments/__init__.py\n",
            "src/baselines/train_tfidf_lr.py\n",
            "src/eval/__init__.py\n",
            "src/eval/eval_pairs.py\n",
            "src/eval/evaluate.py\n",
            "src/transformers/hf_dataset.py\n",
            "src/transformers/train_transformer.py\n",
            "src/__init__.py\n",
            "src/models/gated_fusion.py\n",
            "src/models/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/train/train_transformer.py -h"
      ],
      "metadata": {
        "id": "qeNgTJi41z_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -n '1,80p' src/train/train_transformer.py"
      ],
      "metadata": {
        "id": "An5MeN-p15iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/train/train_transformer.py --config configs/transformer_text_only.yaml --train_file data/train.En.processed.csv"
      ],
      "metadata": {
        "id": "GLIPUjyK18AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/train/train_transformer.py -h"
      ],
      "metadata": {
        "id": "QMQ31aBY1_31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tail -n 40 src/train/train_transformer.py"
      ],
      "metadata": {
        "id": "C4QNvFuk2L-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "cfg = {\n",
        "    \"run_name\": \"roberta_text_only\",\n",
        "    \"model_name\": \"roberta-base\",\n",
        "    \"variant\": \"text\",          # we‚Äôll map this to text_only via columns (see below)\n",
        "    \"max_len\": 128,\n",
        "    \"lr\": 2e-5,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"epochs\": 4,\n",
        "    \"batch_size\": 16,\n",
        "    \"warmup_ratio\": 0.1,\n",
        "    \"seed\": 42,\n",
        "    \"patience\": 2\n",
        "}\n",
        "\n",
        "Path(\"configs\").mkdir(exist_ok=True)\n",
        "Path(\"configs/roberta_text_only.json\").write_text(json.dumps(cfg, indent=2))\n",
        "print(\"Wrote configs/roberta_text_only.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGgKVG_i2mup",
        "outputId": "a2b5dd40-0ceb-425a-f00d-9e3a0fd4eb37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote configs/roberta_text_only.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"src/experiments/common.py\")\n",
        "txt = p.read_text(encoding=\"utf-8\")\n",
        "\n",
        "txt = txt.replace('train_file: str = \"train.En.csv\"', 'train_file: str = \"train.En.processed.csv\"')\n",
        "p.write_text(txt, encoding=\"utf-8\")\n",
        "\n",
        "print(\"Updated default train_file to train.En.processed.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKFl-9n_2rGU",
        "outputId": "874ea7bb-35c6-44e3-834b-8c1ecb7782a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated default train_file to train.En.processed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -n \"train_file\" src/experiments/common.py | head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvHofqQU2t6h",
        "outputId": "4edcc99a-11bf-4e16-ebb8-aac713a7f861"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19:    train_file: str = \"train.En.processed.csv\",\n",
            "24:    df = pd.read_csv(data_dir / train_file)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/transformers/train_transformer.py --config configs/roberta_text_only.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBBwfcPl2wof",
        "outputId": "741e740c-a9e7-4304-e93e-5ee61d836d6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/emoji-aware-sarcasm-nlu/emoji-aware-sarcasm-nlu-main/src/transformers/train_transformer.py\", line 14, in <module>\n",
            "    from src.experiments.common import get_variant_column, load_splits\n",
            "ModuleNotFoundError: No module named 'src'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/emoji-aware-sarcasm-nlu/emoji-aware-sarcasm-nlu-main\n",
        "!python -m src.transformers.train_transformer --config configs/roberta_text_only.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-gZ2kaA2-1W",
        "outputId": "0fe4b884-0900-40b2-c94d-a92363fdd314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/emoji-aware-sarcasm-nlu/emoji-aware-sarcasm-nlu-main\n",
            "config.json: 100% 481/481 [00:00<00:00, 2.14MB/s]\n",
            "tokenizer_config.json: 100% 25.0/25.0 [00:00<00:00, 149kB/s]\n",
            "vocab.json: 100% 899k/899k [00:00<00:00, 7.81MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 2.74MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 6.52MB/s]\n",
            "model.safetensors: 100% 499M/499M [00:04<00:00, 112MB/s]\n",
            "Loading weights: 100% 197/197 [00:00<00:00, 789.10it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]\n",
            "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: roberta-base\n",
            "Key                             | Status     | \n",
            "--------------------------------+------------+-\n",
            "roberta.embeddings.position_ids | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "lm_head.layer_norm.weight       | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "lm_head.bias                    | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "lm_head.dense.bias              | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "lm_head.layer_norm.bias         | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "lm_head.dense.weight            | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "classifier.out_proj.bias        | \u001b[31mMISSING\u001b[0m    | \n",
            "classifier.dense.bias           | \u001b[31mMISSING\u001b[0m    | \n",
            "classifier.out_proj.weight      | \u001b[31mMISSING\u001b[0m    | \n",
            "classifier.dense.weight         | \u001b[31mMISSING\u001b[0m    | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- \u001b[38;5;208mUNEXPECTED\u001b[0m\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "- \u001b[31mMISSING\u001b[0m\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
            "epoch=1 dev macro_f1=0.4287 acc=0.7505\n",
            "epoch=2 dev macro_f1=0.4811 acc=0.7601\n",
            "epoch=3 dev macro_f1=0.5513 acc=0.7620\n",
            "epoch=4 dev macro_f1=0.6007 acc=0.7466\n",
            "[roberta_text_only] best dev macro_f1=0.6007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "base = json.loads(Path(\"configs/roberta_text_only.json\").read_text())\n",
        "\n",
        "cfg_full = dict(base)\n",
        "cfg_full[\"run_name\"] = \"roberta_full\"\n",
        "cfg_full[\"variant\"] = \"full\"\n",
        "Path(\"configs/roberta_full.json\").write_text(json.dumps(cfg_full, indent=2))\n",
        "\n",
        "cfg_dem = dict(base)\n",
        "cfg_dem[\"run_name\"] = \"roberta_demojized\"\n",
        "cfg_dem[\"variant\"] = \"demojized\"\n",
        "Path(\"configs/roberta_demojized.json\").write_text(json.dumps(cfg_dem, indent=2))\n",
        "\n",
        "print(\"created:\", \"configs/roberta_full.json\", \"configs/roberta_demojized.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NO-OFC6N4L2Q",
        "outputId": "ff6868df-f1da-460e-9b71-d00cef1ddefc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created: configs/roberta_full.json configs/roberta_demojized.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m src.transformers.train_transformer --config configs/roberta_full.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNzB3m0t4OnG",
        "outputId": "a2de7cef-cfa1-4d4f-c52e-7c14f6cc8565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading weights: 100% 197/197 [00:00<00:00, 931.18it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]\n",
            "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: roberta-base\n",
            "Key                             | Status     | \n",
            "--------------------------------+------------+-\n",
            "lm_head.layer_norm.weight       | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "roberta.embeddings.position_ids | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "lm_head.layer_norm.bias         | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "lm_head.bias                    | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "lm_head.dense.bias              | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "lm_head.dense.weight            | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "classifier.out_proj.weight      | \u001b[31mMISSING\u001b[0m    | \n",
            "classifier.out_proj.bias        | \u001b[31mMISSING\u001b[0m    | \n",
            "classifier.dense.weight         | \u001b[31mMISSING\u001b[0m    | \n",
            "classifier.dense.bias           | \u001b[31mMISSING\u001b[0m    | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- \u001b[38;5;208mUNEXPECTED\u001b[0m\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "- \u001b[31mMISSING\u001b[0m\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
            "epoch=1 dev macro_f1=0.4287 acc=0.7505\n",
            "epoch=2 dev macro_f1=0.4982 acc=0.7582\n",
            "epoch=3 dev macro_f1=0.5496 acc=0.7658\n",
            "epoch=4 dev macro_f1=0.6025 acc=0.7562\n",
            "[roberta_full] best dev macro_f1=0.6025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m src.transformers.train_transformer --config configs/roberta_demojized.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDKu5fb457SD",
        "outputId": "40850738-e58b-4dfe-94bf-2dfb2374c4e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading weights: 100% 197/197 [00:00<00:00, 863.66it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight] \n",
            "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: roberta-base\n",
            "Key                             | Status     | \n",
            "--------------------------------+------------+-\n",
            "lm_head.layer_norm.weight       | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "lm_head.bias                    | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "roberta.embeddings.position_ids | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "lm_head.dense.bias              | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "lm_head.layer_norm.bias         | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "lm_head.dense.weight            | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "classifier.dense.bias           | \u001b[31mMISSING\u001b[0m    | \n",
            "classifier.out_proj.weight      | \u001b[31mMISSING\u001b[0m    | \n",
            "classifier.out_proj.bias        | \u001b[31mMISSING\u001b[0m    | \n",
            "classifier.dense.weight         | \u001b[31mMISSING\u001b[0m    | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- \u001b[38;5;208mUNEXPECTED\u001b[0m\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "- \u001b[31mMISSING\u001b[0m\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
            "epoch=1 dev macro_f1=0.4287 acc=0.7505\n",
            "epoch=2 dev macro_f1=0.4960 acc=0.7543\n",
            "epoch=3 dev macro_f1=0.5600 acc=0.7620\n",
            "epoch=4 dev macro_f1=0.5775 acc=0.7409\n",
            "[roberta_demojized] best dev macro_f1=0.5775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import emoji\n",
        "\n",
        "def has_emoji(s: str) -> bool:\n",
        "    s = str(s)\n",
        "    return any(ch in emoji.EMOJI_DATA for ch in s)\n",
        "\n",
        "df = pd.read_csv(\"data/train.En.processed.csv\")\n",
        "\n",
        "with open(\"data/splits.json\") as f:\n",
        "    splits = json.load(f)\n",
        "\n",
        "dev = df[df.index.isin(set(splits[\"dev_ids\"]))].copy()\n",
        "mask = dev[\"tweet\"].apply(has_emoji)\n",
        "\n",
        "print(\"Dev size:\", len(dev))\n",
        "print(\"Dev with emoji:\", mask.sum())\n",
        "print(\"Emoji fraction:\", round(mask.mean(), 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfbDRAq6705Z",
        "outputId": "410745a2-bc53-46a8-80ba-67e48431d2a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev size: 521\n",
            "Dev with emoji: 94\n",
            "Emoji fraction: 0.1804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "def eval_subset(run_name: str, subset_ids):\n",
        "    preds = pd.read_csv(f\"results/{run_name}/preds.csv\")\n",
        "    preds = preds[preds[\"id\"].isin(subset_ids)]\n",
        "    y_true = preds[\"y_true\"].values\n",
        "    y_pred = preds[\"y_pred\"].values\n",
        "    return accuracy_score(y_true, y_pred), f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "subset_ids = set(dev[mask].index.astype(int).tolist())\n",
        "\n",
        "runs = [\"roberta_text_only\", \"roberta_full\", \"roberta_demojized\"]\n",
        "for r in runs:\n",
        "    acc, mf1 = eval_subset(r, subset_ids)\n",
        "    print(f\"{r:18s}  macro_f1={mf1:.4f}  acc={acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk4qHP6S74M2",
        "outputId": "ae2b6883-58ff-44a3-d217-193151579a7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "roberta_text_only   macro_f1=0.6628  acc=0.7660\n",
            "roberta_full        macro_f1=0.7122  acc=0.7872\n",
            "roberta_demojized   macro_f1=0.6546  acc=0.7447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la configs/gated_fusion.yaml\n",
        "!sed -n '1,200p' configs/gated_fusion.yaml\n",
        "!ls -la src/train/train_gated_fusion.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqOUJ7dW8_Pu",
        "outputId": "d7fdf1d6-1367-4b26-ab1c-6ba3342f90bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-rw-r-- 1 root root 112 Feb 11 09:18 configs/gated_fusion.yaml\n",
            "model_type: gated_fusion\n",
            "pretrained_name: roberta-base\n",
            "max_len: 128\n",
            "batch_size: 16\n",
            "lr: 2e-5\n",
            "epochs: 4\n",
            "seed: 42\n",
            "\n",
            "-rw-rw-r-- 1 root root 0 Feb 11 09:18 src/train/train_gated_fusion.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pyyaml"
      ],
      "metadata": {
        "id": "Hleb1eEl9fSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -n '1,200p' src/models/gated_fusion.py"
      ],
      "metadata": {
        "id": "vPT3Gjf39vnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/transformers/gated_fusion.py\n",
        "from __future__ import annotations\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel\n",
        "\n",
        "\n",
        "class GatedFusionClassifier(nn.Module):\n",
        "    def __init__(self, pretrained_name: str, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.enc_a = AutoModel.from_pretrained(pretrained_name)\n",
        "        self.enc_b = AutoModel.from_pretrained(pretrained_name)\n",
        "\n",
        "        hidden = self.enc_a.config.hidden_size\n",
        "        self.gate = nn.Linear(hidden * 2, hidden)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cls = nn.Linear(hidden, 2)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        a_input_ids: torch.Tensor,\n",
        "        a_attention_mask: torch.Tensor,\n",
        "        b_input_ids: torch.Tensor,\n",
        "        b_attention_mask: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        ha = self.enc_a(input_ids=a_input_ids, attention_mask=a_attention_mask).last_hidden_state[:, 0]\n",
        "        hb = self.enc_b(input_ids=b_input_ids, attention_mask=b_attention_mask).last_hidden_state[:, 0]\n",
        "\n",
        "        g = torch.sigmoid(self.gate(torch.cat([ha, hb], dim=-1)))\n",
        "        h = g * ha + (1.0 - g) * hb\n",
        "\n",
        "        h = self.drop(h)\n",
        "        return self.cls(h)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IacFiq_-t2p",
        "outputId": "b03407c5-a152-47b2-e660-686446cd98ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/transformers/gated_fusion.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/train/train_gated_fusion.py\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "from dataclasses import asdict, dataclass\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import yaml\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, get_linear_schedule_with_warmup\n",
        "\n",
        "from src.experiments.common import load_splits\n",
        "from src.experiments.io import ensure_dir, save_json, save_preds_csv\n",
        "from src.experiments.metrics import compute_metrics\n",
        "from src.transformers.gated_fusion import GatedFusionClassifier\n",
        "\n",
        "\n",
        "def set_seed(seed: int) -> None:\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Cfg:\n",
        "    model_type: str\n",
        "    pretrained_name: str\n",
        "    max_len: int\n",
        "    batch_size: int\n",
        "    lr: float\n",
        "    epochs: int\n",
        "    seed: int\n",
        "    run_name: str = \"gated_fusion\"\n",
        "    weight_decay: float = 0.01\n",
        "    warmup_ratio: float = 0.1\n",
        "    patience: int = 2\n",
        "\n",
        "\n",
        "def parse_args() -> argparse.Namespace:\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--config\", required=True)\n",
        "    return p.parse_args()\n",
        "\n",
        "\n",
        "def load_cfg(path: str | Path) -> Cfg:\n",
        "    with Path(path).open(\"r\", encoding=\"utf-8\") as f:\n",
        "        raw = yaml.safe_load(f)\n",
        "    return Cfg(**raw)\n",
        "\n",
        "\n",
        "class DualDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, enc_a, enc_b, labels: List[int], ids: List[int]):\n",
        "        self.enc_a = enc_a\n",
        "        self.enc_b = enc_b\n",
        "        self.labels = labels\n",
        "        self.ids = ids\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        return {\n",
        "            \"a_input_ids\": torch.tensor(self.enc_a[\"input_ids\"][idx]),\n",
        "            \"a_attention_mask\": torch.tensor(self.enc_a[\"attention_mask\"][idx]),\n",
        "            \"b_input_ids\": torch.tensor(self.enc_b[\"input_ids\"][idx]),\n",
        "            \"b_attention_mask\": torch.tensor(self.enc_b[\"attention_mask\"][idx]),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
        "            \"id\": torch.tensor(self.ids[idx], dtype=torch.long),\n",
        "        }\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict(model, loader, device) -> Tuple[np.ndarray, np.ndarray, np.ndarray, List[int]]:\n",
        "    model.eval()\n",
        "    ys, yp, pr, ids = [], [], [], []\n",
        "    for batch in loader:\n",
        "        batch_ids = batch.pop(\"id\").cpu().numpy().tolist()\n",
        "        y = batch.pop(\"labels\").cpu().numpy()\n",
        "\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        logits = model(**batch).detach().cpu().numpy()\n",
        "\n",
        "        probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()[:, 1]\n",
        "        pred = (probs >= 0.5).astype(int)\n",
        "\n",
        "        ys.append(y)\n",
        "        yp.append(pred)\n",
        "        pr.append(probs)\n",
        "        ids.extend(batch_ids)\n",
        "\n",
        "    return np.concatenate(ys), np.concatenate(yp), np.concatenate(pr), ids\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    args = parse_args()\n",
        "    cfg = load_cfg(args.config)\n",
        "\n",
        "    set_seed(cfg.seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    splits = load_splits()  # should default to train.En.processed.csv from your earlier patch\n",
        "\n",
        "    df_tr = splits.train\n",
        "    df_dv = splits.dev\n",
        "\n",
        "    x_text_tr = df_tr[\"text_only\"].astype(str).tolist()\n",
        "    x_emoji_tr = df_tr[\"emoji_only\"].astype(str).tolist()\n",
        "    y_tr = df_tr[\"sarcastic\"].astype(int).tolist()\n",
        "    ids_tr = df_tr.index.astype(int).tolist()\n",
        "\n",
        "    x_text_dv = df_dv[\"text_only\"].astype(str).tolist()\n",
        "    x_emoji_dv = df_dv[\"emoji_only\"].astype(str).tolist()\n",
        "    y_dv = df_dv[\"sarcastic\"].astype(int).tolist()\n",
        "    ids_dv = df_dv.index.astype(int).tolist()\n",
        "\n",
        "    tok = AutoTokenizer.from_pretrained(cfg.pretrained_name, use_fast=True)\n",
        "\n",
        "    enc_text_tr = tok(x_text_tr, truncation=True, padding=True, max_length=cfg.max_len)\n",
        "    enc_emoji_tr = tok(x_emoji_tr, truncation=True, padding=True, max_length=cfg.max_len)\n",
        "    enc_text_dv = tok(x_text_dv, truncation=True, padding=True, max_length=cfg.max_len)\n",
        "    enc_emoji_dv = tok(x_emoji_dv, truncation=True, padding=True, max_length=cfg.max_len)\n",
        "\n",
        "    ds_tr = DualDataset(enc_text_tr, enc_emoji_tr, y_tr, ids_tr)\n",
        "    ds_dv = DualDataset(enc_text_dv, enc_emoji_dv, y_dv, ids_dv)\n",
        "\n",
        "    dl_tr = DataLoader(ds_tr, batch_size=cfg.batch_size, shuffle=True)\n",
        "    dl_dv = DataLoader(ds_dv, batch_size=cfg.batch_size, shuffle=False)\n",
        "\n",
        "    model = GatedFusionClassifier(cfg.pretrained_name)\n",
        "    model.to(device)\n",
        "\n",
        "    optim = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "\n",
        "    total_steps = len(dl_tr) * cfg.epochs\n",
        "    warmup_steps = int(total_steps * cfg.warmup_ratio)\n",
        "    sched = get_linear_schedule_with_warmup(optim, warmup_steps, total_steps)\n",
        "\n",
        "    best_f1 = -1.0\n",
        "    best_state = None\n",
        "    bad = 0\n",
        "\n",
        "    for epoch in range(cfg.epochs):\n",
        "        model.train()\n",
        "        for batch in dl_tr:\n",
        "            batch.pop(\"id\")\n",
        "            labels = batch.pop(\"labels\").to(device)\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            logits = model(**batch)\n",
        "            loss = torch.nn.functional.cross_entropy(logits, labels)\n",
        "\n",
        "            optim.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optim.step()\n",
        "            sched.step()\n",
        "\n",
        "        y_true, y_pred, probs, ids = predict(model, dl_dv, device)\n",
        "        m = compute_metrics(y_true, y_pred)\n",
        "        print(f\"epoch={epoch+1} dev macro_f1={m.macro_f1:.4f} acc={m.accuracy:.4f}\")\n",
        "\n",
        "        if m.macro_f1 > best_f1 + 1e-6:\n",
        "            best_f1 = m.macro_f1\n",
        "            best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
        "            bad = 0\n",
        "        else:\n",
        "            bad += 1\n",
        "            if bad >= cfg.patience:\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    y_true, y_pred, probs, ids = predict(model, dl_dv, device)\n",
        "    m = compute_metrics(y_true, y_pred)\n",
        "\n",
        "    out_dir = ensure_dir(Path(\"results\") / cfg.run_name)\n",
        "    save_json(out_dir / \"metrics.json\", {\"accuracy\": m.accuracy, \"macro_f1\": m.macro_f1, \"per_class\": m.per_class})\n",
        "    save_json(out_dir / \"confusion.json\", {\"labels\": [0, 1], \"matrix\": m.confusion})\n",
        "    save_preds_csv(out_dir / \"preds.csv\", ids, y_true.tolist(), y_pred.tolist(), probs.tolist())\n",
        "    save_json(out_dir / \"config.json\", asdict(cfg))\n",
        "\n",
        "    print(f\"[{cfg.run_name}] best dev macro_f1={m.macro_f1:.4f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tkQWcnV_BCh",
        "outputId": "de52630b-f863-4af9-d8f8-57a6270480b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting src/train/train_gated_fusion.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "p = Path(\"src/train/train_gated_fusion.py\")\n",
        "txt = p.read_text(encoding=\"utf-8\")\n",
        "\n",
        "pattern = r\"def load_cfg\\(path: str \\| Path\\) -> Cfg:\\n(.*?)\\n\\n\"\n",
        "if \"raw = yaml.safe_load\" not in txt:\n",
        "    print(\"Couldn't find load_cfg block, open the file to patch manually.\")\n",
        "else:\n",
        "    new_block = \"\"\"def load_cfg(path: str | Path) -> Cfg:\n",
        "    with Path(path).open(\"r\", encoding=\"utf-8\") as f:\n",
        "        raw = yaml.safe_load(f)\n",
        "\n",
        "    # YAML sometimes parses scientific notation as strings in Colab.\n",
        "    for k in (\"max_len\", \"batch_size\", \"epochs\", \"seed\", \"patience\"):\n",
        "        if k in raw:\n",
        "            raw[k] = int(raw[k])\n",
        "    for k in (\"lr\", \"weight_decay\", \"warmup_ratio\"):\n",
        "        if k in raw:\n",
        "            raw[k] = float(raw[k])\n",
        "\n",
        "    return Cfg(**raw)\n",
        "\"\"\"\n",
        "    import textwrap\n",
        "    lines = txt.splitlines()\n",
        "    out = []\n",
        "    i = 0\n",
        "    while i < len(lines):\n",
        "        line = lines[i]\n",
        "        if line.startswith(\"def load_cfg(\"):\n",
        "            out.append(new_block.rstrip(\"\\n\"))\n",
        "            i += 1\n",
        "            while i < len(lines) and not lines[i].startswith(\"def \"):\n",
        "                i += 1\n",
        "            continue\n",
        "        out.append(line)\n",
        "        i += 1\n",
        "\n",
        "    p.write_text(\"\\n\".join(out) + \"\\n\", encoding=\"utf-8\")\n",
        "    print(\"Patched load_cfg() to cast YAML numeric fields.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vb13inKFAGvf",
        "outputId": "1b6b834e-347f-41f4-d609-6534b37161bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patched load_cfg() to cast YAML numeric fields.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -n \"def load_cfg\" -n -A 25 src/train/train_gated_fusion.py | head -n 35"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTKfuTBGAMGu",
        "outputId": "7834f304-900d-4613-be67-60e74655bece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47:def load_cfg(path: str | Path) -> Cfg:\n",
            "48-    with Path(path).open(\"r\", encoding=\"utf-8\") as f:\n",
            "49-        raw = yaml.safe_load(f)\n",
            "50-\n",
            "51-    # YAML sometimes parses scientific notation as strings in Colab.\n",
            "52-    for k in (\"max_len\", \"batch_size\", \"epochs\", \"seed\", \"patience\"):\n",
            "53-        if k in raw:\n",
            "54-            raw[k] = int(raw[k])\n",
            "55-    for k in (\"lr\", \"weight_decay\", \"warmup_ratio\"):\n",
            "56-        if k in raw:\n",
            "57-            raw[k] = float(raw[k])\n",
            "58-\n",
            "59-    return Cfg(**raw)\n",
            "60-def predict(model, loader, device) -> Tuple[np.ndarray, np.ndarray, np.ndarray, List[int]]:\n",
            "61-    model.eval()\n",
            "62-    ys, yp, pr, ids = [], [], [], []\n",
            "63-    for batch in loader:\n",
            "64-        batch_ids = batch.pop(\"id\").cpu().numpy().tolist()\n",
            "65-        y = batch.pop(\"labels\").cpu().numpy()\n",
            "66-\n",
            "67-        batch = {k: v.to(device) for k, v in batch.items()}\n",
            "68-        logits = model(**batch).detach().cpu().numpy()\n",
            "69-\n",
            "70-        probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()[:, 1]\n",
            "71-        pred = (probs >= 0.5).astype(int)\n",
            "72-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -n \"class Dual\" -n src/train/train_gated_fusion.py\n",
        "!grep -n \"DualDataset\" -n src/train/train_gated_fusion.py | head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fh9EErMqAs2S",
        "outputId": "234990d5-96fa-46e1-f4ae-c559d99708c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "110:    ds_tr = DualDataset(enc_text_tr, enc_emoji_tr, y_tr, ids_tr)\n",
            "111:    ds_dv = DualDataset(enc_text_dv, enc_emoji_dv, y_dv, ids_dv)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"src/train/train_gated_fusion.py\")\n",
        "txt = p.read_text(encoding=\"utf-8\")\n",
        "\n",
        "marker = \"return Cfg(**raw)\\n\"\n",
        "if marker not in txt:\n",
        "    raise ValueError(\"Could not find insertion point after load_cfg().\")\n",
        "\n",
        "dataset_code = marker + \"\"\"\n",
        "class DualDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, enc_a, enc_b, labels, ids):\n",
        "        self.enc_a = enc_a\n",
        "        self.enc_b = enc_b\n",
        "        self.labels = labels\n",
        "        self.ids = ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"a_input_ids\": torch.tensor(self.enc_a[\"input_ids\"][idx]),\n",
        "            \"a_attention_mask\": torch.tensor(self.enc_a[\"attention_mask\"][idx]),\n",
        "            \"b_input_ids\": torch.tensor(self.enc_b[\"input_ids\"][idx]),\n",
        "            \"b_attention_mask\": torch.tensor(self.enc_b[\"attention_mask\"][idx]),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
        "            \"id\": torch.tensor(self.ids[idx], dtype=torch.long),\n",
        "        }\n",
        "\"\"\"\n",
        "\n",
        "# avoid inserting twice\n",
        "if \"class DualDataset\" not in txt:\n",
        "    txt = txt.replace(marker, dataset_code)\n",
        "    p.write_text(txt, encoding=\"utf-8\")\n",
        "    print(\"Inserted DualDataset class.\")\n",
        "else:\n",
        "    print(\"DualDataset already exists somewhere in file.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbgzYv7ZBk9M",
        "outputId": "01da095c-475c-472d-d54f-af8aa3b81149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inserted DualDataset class.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -n \"class DualDataset\" -n src/train/train_gated_fusion.py\n",
        "!grep -n \"def main\" -n src/train/train_gated_fusion.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiEOlbtyBnuR",
        "outputId": "44fa441f-9c6c-4aff-f1fd-a58a5671b117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "61:class DualDataset(torch.utils.data.Dataset):\n",
            "101:def main() -> None:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m src.train.train_gated_fusion --config configs/gated_fusion.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuRSwmOMB17Y",
        "outputId": "533b4e64-d5b2-4131-d696-d1e825830cf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "Loading weights: 100% 197/197 [00:00<00:00, 896.91it/s, Materializing param=encoder.layer.11.output.dense.weight]\n",
            "\u001b[1mRobertaModel LOAD REPORT\u001b[0m from: roberta-base\n",
            "Key                             | Status     | \n",
            "--------------------------------+------------+-\n",
            "lm_head.bias                    | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "lm_head.dense.weight            | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "lm_head.layer_norm.weight       | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "lm_head.layer_norm.bias         | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "roberta.embeddings.position_ids | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "lm_head.dense.bias              | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "pooler.dense.weight             | \u001b[31mMISSING\u001b[0m    | \n",
            "pooler.dense.bias               | \u001b[31mMISSING\u001b[0m    | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- \u001b[38;5;208mUNEXPECTED\u001b[0m\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "- \u001b[31mMISSING\u001b[0m\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
            "Loading weights: 100% 197/197 [00:00<00:00, 1485.43it/s, Materializing param=encoder.layer.11.output.dense.weight]\n",
            "\u001b[1mRobertaModel LOAD REPORT\u001b[0m from: roberta-base\n",
            "Key                             | Status     | \n",
            "--------------------------------+------------+-\n",
            "lm_head.bias                    | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "lm_head.dense.weight            | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "lm_head.layer_norm.weight       | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "lm_head.layer_norm.bias         | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "roberta.embeddings.position_ids | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "lm_head.dense.bias              | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "pooler.dense.weight             | \u001b[31mMISSING\u001b[0m    | \n",
            "pooler.dense.bias               | \u001b[31mMISSING\u001b[0m    | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- \u001b[38;5;208mUNEXPECTED\u001b[0m\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "- \u001b[31mMISSING\u001b[0m\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
            "epoch=1 dev macro_f1=0.4287 acc=0.7505\n",
            "epoch=2 dev macro_f1=0.4519 acc=0.7543\n",
            "epoch=3 dev macro_f1=0.5392 acc=0.7716\n",
            "epoch=4 dev macro_f1=0.5872 acc=0.7620\n",
            "[gated_fusion] best dev macro_f1=0.5872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la results/gated_fusion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYWTE--ODcdZ",
        "outputId": "3c5e0ecf-da3b-40cd-aa0c-e2caaf931817"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 36\n",
            "drwxr-xr-x  2 root root  4096 Feb 15 16:05 .\n",
            "drwxrwxr-x 10 root root  4096 Feb 15 16:05 ..\n",
            "-rw-r--r--  1 root root   247 Feb 15 16:05 config.json\n",
            "-rw-r--r--  1 root root   118 Feb 15 16:05 confusion.json\n",
            "-rw-r--r--  1 root root   387 Feb 15 16:05 metrics.json\n",
            "-rw-r--r--  1 root root 15132 Feb 15 16:05 preds.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, os\n",
        "\n",
        "runs = [\n",
        "    \"lr_tfidf_text_proc\",\n",
        "    \"lr_tfidf_demojized_proc\",\n",
        "    \"roberta_text_only\",\n",
        "    \"roberta_full\",\n",
        "    \"roberta_demojized\",\n",
        "    \"gated_fusion\",\n",
        "]\n",
        "\n",
        "for r in runs:\n",
        "    p = f\"results/{r}/metrics.json\"\n",
        "    if not os.path.exists(p):\n",
        "        print(r, \"-> missing\")\n",
        "        continue\n",
        "    m = json.load(open(p))\n",
        "    print(f\"{r:20s} macro_f1={m['macro_f1']:.4f}  acc={m['accuracy']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--TEPidODhNS",
        "outputId": "1e45dbf7-cc0b-4d17-d36f-a272dd6743e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr_tfidf_text_proc   macro_f1=0.5672  acc=0.6967\n",
            "lr_tfidf_demojized_proc macro_f1=0.5687  acc=0.6987\n",
            "roberta_text_only    macro_f1=0.6007  acc=0.7466\n",
            "roberta_full         macro_f1=0.6025  acc=0.7562\n",
            "roberta_demojized    macro_f1=0.5775  acc=0.7409\n",
            "gated_fusion         macro_f1=0.5872  acc=0.7620\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/transformers/gated_fusion.py\n",
        "from __future__ import annotations\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel\n",
        "\n",
        "\n",
        "class GatedFusionClassifier(nn.Module):\n",
        "    def __init__(self, pretrained_name: str, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.enc = AutoModel.from_pretrained(pretrained_name)\n",
        "\n",
        "        hidden = self.enc.config.hidden_size\n",
        "        self.gate = nn.Linear(hidden * 2, hidden)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cls = nn.Linear(hidden, 2)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        a_input_ids: torch.Tensor,\n",
        "        a_attention_mask: torch.Tensor,\n",
        "        b_input_ids: torch.Tensor,\n",
        "        b_attention_mask: torch.Tensor,\n",
        "        emoji_present: torch.Tensor | None = None,\n",
        "    ) -> torch.Tensor:\n",
        "        ha = self.enc(input_ids=a_input_ids, attention_mask=a_attention_mask).last_hidden_state[:, 0]\n",
        "        hb = self.enc(input_ids=b_input_ids, attention_mask=b_attention_mask).last_hidden_state[:, 0]\n",
        "\n",
        "        if emoji_present is None:\n",
        "            emoji_present = torch.ones((ha.size(0),), device=ha.device, dtype=ha.dtype)\n",
        "        else:\n",
        "            emoji_present = emoji_present.to(device=ha.device, dtype=ha.dtype)\n",
        "\n",
        "        g = torch.sigmoid(self.gate(torch.cat([ha, hb], dim=-1)))\n",
        "        emoji_present = emoji_present.view(-1, 1)\n",
        "        g = emoji_present * g + (1.0 - emoji_present) * 1.0\n",
        "\n",
        "        h = g * ha + (1.0 - g) * hb\n",
        "        h = self.drop(h)\n",
        "        return self.cls(h)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOlSAQDWEZT-",
        "outputId": "1339dd85-d9fd-4f28-f3d5-e8dc7570d8a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting src/transformers/gated_fusion.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "p = Path(\"src/train/train_gated_fusion.py\")\n",
        "txt = p.read_text(encoding=\"utf-8\")\n",
        "\n",
        "# Insert emoji_present lists right after x_emoji_* are created\n",
        "txt = txt.replace(\n",
        "    'x_emoji_tr = df_tr[\"emoji_only\"].astype(str).tolist()',\n",
        "    'x_emoji_tr = df_tr[\"emoji_only\"].astype(str).tolist()\\n    emoji_present_tr = [1 if s.strip() else 0 for s in x_emoji_tr]'\n",
        ")\n",
        "\n",
        "txt = txt.replace(\n",
        "    'x_emoji_dv = df_dv[\"emoji_only\"].astype(str).tolist()',\n",
        "    'x_emoji_dv = df_dv[\"emoji_only\"].astype(str).tolist()\\n    emoji_present_dv = [1 if s.strip() else 0 for s in x_emoji_dv]'\n",
        ")\n",
        "\n",
        "p.write_text(txt, encoding=\"utf-8\")\n",
        "print(\"Added emoji_present_tr/dev lists.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3RTFahdEiqY",
        "outputId": "2fc32a6e-a858-484e-f61c-6d07e27eb81f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added emoji_present_tr/dev lists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"src/train/train_gated_fusion.py\")\n",
        "txt = p.read_text(encoding=\"utf-8\")\n",
        "\n",
        "txt = txt.replace(\n",
        "    \"def __init__(self, enc_a, enc_b, labels, ids):\",\n",
        "    \"def __init__(self, enc_a, enc_b, labels, ids, emoji_present):\"\n",
        ")\n",
        "txt = txt.replace(\n",
        "    \"self.ids = ids\",\n",
        "    \"self.ids = ids\\n        self.emoji_present = emoji_present\"\n",
        ")\n",
        "\n",
        "# add field in __getitem__\n",
        "needle = '\"id\": torch.tensor(self.ids[idx], dtype=torch.long),'\n",
        "if needle in txt and '\"emoji_present\"' not in txt:\n",
        "    txt = txt.replace(\n",
        "        needle,\n",
        "        needle + '\\n            \"emoji_present\": torch.tensor(self.emoji_present[idx], dtype=torch.float),'\n",
        "    )\n",
        "\n",
        "p.write_text(txt, encoding=\"utf-8\")\n",
        "print(\"Updated DualDataset to include emoji_present.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnnNe8NpEl_6",
        "outputId": "284e93de-251a-4fd9-9536-b749989362c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated DualDataset to include emoji_present.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"src/train/train_gated_fusion.py\")\n",
        "txt = p.read_text(encoding=\"utf-8\")\n",
        "\n",
        "txt = txt.replace(\n",
        "    \"ds_tr = DualDataset(enc_text_tr, enc_emoji_tr, y_tr, ids_tr)\",\n",
        "    \"ds_tr = DualDataset(enc_text_tr, enc_emoji_tr, y_tr, ids_tr, emoji_present_tr)\"\n",
        ")\n",
        "txt = txt.replace(\n",
        "    \"ds_dv = DualDataset(enc_text_dv, enc_emoji_dv, y_dv, ids_dv)\",\n",
        "    \"ds_dv = DualDataset(enc_text_dv, enc_emoji_dv, y_dv, ids_dv, emoji_present_dv)\"\n",
        ")\n",
        "\n",
        "p.write_text(txt, encoding=\"utf-8\")\n",
        "print(\"Passed emoji_present into DualDataset.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3-kh1mnEqMq",
        "outputId": "3dd95fc5-9674-4c00-dc8d-12020b72b38b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed emoji_present into DualDataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"src/train/train_gated_fusion.py\")\n",
        "txt = p.read_text(encoding=\"utf-8\")\n",
        "\n",
        "# In predict(), don't pop emoji_present; keep it in batch and send to model\n",
        "# In training loop, batch already becomes dict -> model(**batch) will receive emoji_present\n",
        "\n",
        "# But we must ensure labels/id are removed properly and emoji_present stays.\n",
        "# If your training loop pops labels then does batch = {k: v.to(device)}, it's fine.\n",
        "\n",
        "p.write_text(txt, encoding=\"utf-8\")\n",
        "print(\"No extra change needed if model(**batch) is used with emoji_present in batch.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oR4o8Tp1Et9D",
        "outputId": "9ad231c0-e0aa-4627-f3fc-0a38e55e0e1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No extra change needed if model(**batch) is used with emoji_present in batch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -n \"emoji_present\" -n src/train/train_gated_fusion.py | head -n 40"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TE-HyAkhExNn",
        "outputId": "26f814e7-bebd-4b93-8214-ce1ecc9ea95c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "62:    def __init__(self, enc_a, enc_b, labels, ids, emoji_present):\n",
            "67:        self.emoji_present = emoji_present\n",
            "80:            \"emoji_present\": torch.tensor(self.emoji_present[idx], dtype=torch.float),\n",
            "117:    emoji_present_tr = [1 if s.strip() else 0 for s in x_emoji_tr]\n",
            "123:    emoji_present_dv = [1 if s.strip() else 0 for s in x_emoji_dv]\n",
            "134:    ds_tr = DualDataset(enc_text_tr, enc_emoji_tr, y_tr, ids_tr, emoji_present_tr)\n",
            "135:    ds_dv = DualDataset(enc_text_dv, enc_emoji_dv, y_dv, ids_dv, emoji_present_dv)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_name: gated_fusion_shared_masked"
      ],
      "metadata": {
        "id": "4yjbTbWDE0Yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile configs/gated_fusion.yaml\n",
        "model_type: gated_fusion\n",
        "run_name: gated_fusion_shared_masked\n",
        "pretrained_name: roberta-base\n",
        "max_len: 128\n",
        "batch_size: 16\n",
        "lr: 2e-5\n",
        "epochs: 4\n",
        "sid: 42"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcR8QSQfHi9d",
        "outputId": "4eae13f7-e7d4-4ce1-f7a4-0e0f54ec7671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting configs/gated_fusion.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile configs/gated_fusion.yaml\n",
        "model_type: gated_fusion\n",
        "run_name: gated_fusion_shared_masked\n",
        "pretrained_name: roberta-base\n",
        "max_len: 128\n",
        "batch_size: 16\n",
        "lr: 2e-5\n",
        "epochs: 4\n",
        "seed: 42"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b-vHGFvJJNE",
        "outputId": "5a3608bc-1c13-44d6-f34e-c3762c91f0dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting configs/gated_fusion.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m src.train.train_gated_fusion --config configs/gated_fusion.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8VTJK1eIbCP",
        "outputId": "8c32286e-5fdb-4906-d82d-d45e5e736fd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading weights: 100% 197/197 [00:00<00:00, 1116.50it/s, Materializing param=encoder.layer.11.output.dense.weight]\n",
            "\u001b[1mRobertaModel LOAD REPORT\u001b[0m from: roberta-base\n",
            "Key                             | Status     | \n",
            "--------------------------------+------------+-\n",
            "lm_head.bias                    | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "lm_head.dense.bias              | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "roberta.embeddings.position_ids | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "lm_head.layer_norm.bias         | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "lm_head.dense.weight            | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "lm_head.layer_norm.weight       | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "pooler.dense.weight             | \u001b[31mMISSING\u001b[0m    | \n",
            "pooler.dense.bias               | \u001b[31mMISSING\u001b[0m    | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- \u001b[38;5;208mUNEXPECTED\u001b[0m\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "- \u001b[31mMISSING\u001b[0m\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
            "epoch=1 dev macro_f1=0.4287 acc=0.7505\n",
            "epoch=2 dev macro_f1=0.4527 acc=0.7562\n",
            "epoch=3 dev macro_f1=0.5687 acc=0.7428\n",
            "epoch=4 dev macro_f1=0.5687 acc=0.7524\n",
            "[gated_fusion_shared_masked] best dev macro_f1=0.5687\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import emoji\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "def has_emoji(s: str) -> bool:\n",
        "    s = str(s)\n",
        "    return any(ch in emoji.EMOJI_DATA for ch in s)\n",
        "\n",
        "df = pd.read_csv(\"data/train.En.processed.csv\")\n",
        "with open(\"data/splits.json\") as f:\n",
        "    splits = json.load(f)\n",
        "\n",
        "dev = df[df.index.isin(set(splits[\"dev_ids\"]))].copy()\n",
        "subset_ids = set(dev[dev[\"tweet\"].apply(has_emoji)].index.astype(int).tolist())\n",
        "\n",
        "def eval_subset(run_name: str):\n",
        "    preds = pd.read_csv(f\"results/{run_name}/preds.csv\")\n",
        "    preds = preds[preds[\"id\"].isin(subset_ids)]\n",
        "    y_true = preds[\"y_true\"].values\n",
        "    y_pred = preds[\"y_pred\"].values\n",
        "    return accuracy_score(y_true, y_pred), f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "for r in [\"roberta_full\", \"gated_fusion\", \"gated_fusion_shared_masked\"]:\n",
        "    acc, mf1 = eval_subset(r)\n",
        "    print(f\"{r:28s} macro_f1={mf1:.4f} acc={acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMKJ_CjDKx8N",
        "outputId": "5cb22b93-bbd6-4b56-9fc3-d1b2f2dee7c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "roberta_full                 macro_f1=0.7122 acc=0.7872\n",
            "gated_fusion                 macro_f1=0.6130 acc=0.7553\n",
            "gated_fusion_shared_masked   macro_f1=0.6189 acc=0.7447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJac5qctL9T4",
        "outputId": "ea4787e7-1064-412c-e1c4-ee5779992982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/emoji-aware-sarcasm-nlu/emoji-aware-sarcasm-nlu-main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/emoji-aware-sarcasm-nlu/emoji-aware-sarcasm-nlu-main \\\n",
        "/content/drive/MyDrive/emoji-aware-sarcasm-nlu-backup"
      ],
      "metadata": {
        "id": "geaE3yEpMFwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/emoji-aware-sarcasm-nlu-backup/results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TqtpiS4MI1Y",
        "outputId": "495aba11-da1b-4b55-e527-ae5682914bab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gated_fusion\t\t    lr_tfidf_text\troberta_full\n",
            "gated_fusion_shared_masked  lr_tfidf_text_proc\troberta_text_only\n",
            "lr_tfidf_demojized\t    README_results.md\n",
            "lr_tfidf_demojized_proc     roberta_demojized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9dB35QENH-R",
        "outputId": "1e050efa-8f73-471a-fa79-95c7a9e3d3b3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/emoji-aware-sarcasm-nlu-backup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71QXWPqKNaK4",
        "outputId": "259a6bb3-2b14-4a95-b1b4-1a4fd40b9b34"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/emoji-aware-sarcasm-nlu-backup'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2UhPdpdNl6a",
        "outputId": "b05a791a-08da-4e30-9075-4d9f91bd9004"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUi3zwXRNy20",
        "outputId": "6ba91e51-6a1d-4361-cc7a-9d52d3ef44f4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1712818096534.jpg\n",
            "'31996-23 aswathyramesh80@gmail.com_sig.pdf'\n",
            " aswathyresume.docx\n",
            " Backup\n",
            "'[#bb578]__FID__transaction 2025-02-25T13_00_04.pdf'\n",
            "'Colab Notebooks'\n",
            "'Copy of Untitled presentation.gslides'\n",
            " Criteria_For_Reliable_Resources\n",
            " data\n",
            " data.zip\n",
            " emoji-aware-sarcasm-nlu-backup\n",
            "'Form I -Gratuity form-Aswathy1022544.pdf'\n",
            " Germany\n",
            "'Google AI Studio'\n",
            "'How to get started with Drive.pdf'\n",
            " ProgressVideo1.gif\n",
            "'QBurst: Interview Call Letter - 23 June 2018, Saturday.pdf'\n",
            " RentalProperty\n",
            "'Screen Recording 2025-10-21 at 9.22.55‚ÄØPM.mov'\n",
            "'Screen Recording 2025-11-05 at 9.23.39‚ÄØPM.mov'\n",
            "'To-do list.gsheet'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/MyDrive/emoji-aware-sarcasm-nlu-backup\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKe9FS7wOHQj",
        "outputId": "a34d06e1-7ba4-4621-c295-3e98f4d65cfc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/emoji-aware-sarcasm-nlu-backup\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpDBI5wmOLW0",
        "outputId": "5115a959-2c0f-40db-8a3c-144d4bb643c0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/emoji-aware-sarcasm-nlu-backup\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Uc8xersOOy3",
        "outputId": "d7429a1b-7ccd-4c4b-8419-ec60d5cb107a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gated_fusion\t\t    lr_tfidf_text\troberta_full\n",
            "gated_fusion_shared_masked  lr_tfidf_text_proc\troberta_text_only\n",
            "lr_tfidf_demojized\t    README_results.md\n",
            "lr_tfidf_demojized_proc     roberta_demojized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n",
        "import json\n",
        "import pandas as pd\n",
        "import emoji\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "def has_emoji(s: str) -> bool:\n",
        "    s = str(s)\n",
        "    return any(ch in emoji.EMOJI_DATA for ch in s)\n",
        "\n",
        "df = pd.read_csv(\"data/train.En.processed.csv\")\n",
        "\n",
        "with open(\"data/splits.json\") as f:\n",
        "    splits = json.load(f)\n",
        "\n",
        "dev = df[df.index.isin(set(splits[\"dev_ids\"]))].copy()\n",
        "emoji_dev = dev[dev[\"tweet\"].apply(has_emoji)].copy()\n",
        "\n",
        "subset_ids = set(emoji_dev.index.astype(int).tolist())\n",
        "\n",
        "print(\"Total dev samples:\", len(dev))\n",
        "print(\"Emoji-only dev samples:\", len(emoji_dev))\n",
        "print()\n",
        "\n",
        "def eval_subset(run_name: str):\n",
        "    preds = pd.read_csv(f\"results/{run_name}/preds.csv\")\n",
        "    preds = preds[preds[\"id\"].isin(subset_ids)]\n",
        "    y_true = preds[\"y_true\"].values\n",
        "    y_pred = preds[\"y_pred\"].values\n",
        "    return accuracy_score(y_true, y_pred), f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "runs = [\n",
        "    \"lr_tfidf_text_proc\",\n",
        "    \"lr_tfidf_demojized_proc\",\n",
        "    \"roberta_text_only\",\n",
        "    \"roberta_full\",\n",
        "    \"roberta_demojized\",\n",
        "    \"gated_fusion\",\n",
        "    \"gated_fusion_shared_masked\",\n",
        "]\n",
        "\n",
        "print(\"=== Emoji-only Subset Results ===\")\n",
        "for r in runs:\n",
        "    acc, mf1 = eval_subset(r)\n",
        "    print(f\"{r:28s} macro_f1={mf1:.4f}  acc={acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6c8YJaMOSxD",
        "outputId": "147d99ba-ce5a-4aa3-a76d-499dbb48ea97"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/608.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.15.0\n",
            "Total dev samples: 521\n",
            "Emoji-only dev samples: 94\n",
            "\n",
            "=== Emoji-only Subset Results ===\n",
            "lr_tfidf_text_proc           macro_f1=0.5927  acc=0.6702\n",
            "lr_tfidf_demojized_proc      macro_f1=0.6439  acc=0.7021\n",
            "roberta_text_only            macro_f1=0.6628  acc=0.7660\n",
            "roberta_full                 macro_f1=0.7122  acc=0.7872\n",
            "roberta_demojized            macro_f1=0.6546  acc=0.7447\n",
            "gated_fusion                 macro_f1=0.6130  acc=0.7553\n",
            "gated_fusion_shared_masked   macro_f1=0.6189  acc=0.7447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "runs = [\n",
        "    \"lr_tfidf_text_proc\",\n",
        "    \"lr_tfidf_demojized_proc\",\n",
        "    \"roberta_text_only\",\n",
        "    \"roberta_full\",\n",
        "    \"roberta_demojized\",\n",
        "    \"gated_fusion\",\n",
        "    \"gated_fusion_shared_masked\",\n",
        "]\n",
        "\n",
        "print(\"=== Full Dev Set Results ===\\n\")\n",
        "\n",
        "print(f\"{'Model':28s} {'Macro-F1':>10s} {'Accuracy':>10s}\")\n",
        "print(\"-\" * 52)\n",
        "\n",
        "for r in runs:\n",
        "    path = f\"results/{r}/metrics.json\"\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"{r:28s} {'MISSING':>10s} {'MISSING':>10s}\")\n",
        "        continue\n",
        "\n",
        "    with open(path) as f:\n",
        "        m = json.load(f)\n",
        "\n",
        "    print(f\"{r:28s} {m['macro_f1']:.4f} {m['accuracy']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TL9TQLSRQk5v",
        "outputId": "eb763a1f-f068-4097-9bc7-13cd64ffb1a6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Full Dev Set Results ===\n",
            "\n",
            "Model                          Macro-F1   Accuracy\n",
            "----------------------------------------------------\n",
            "lr_tfidf_text_proc           0.5672 0.6967\n",
            "lr_tfidf_demojized_proc      0.5687 0.6987\n",
            "roberta_text_only            0.6007 0.7466\n",
            "roberta_full                 0.6025 0.7562\n",
            "roberta_demojized            0.5775 0.7409\n",
            "gated_fusion                 0.5872 0.7620\n",
            "gated_fusion_shared_masked   0.5687 0.7524\n"
          ]
        }
      ]
    }
  ]
}