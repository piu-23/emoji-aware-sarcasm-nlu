# Emoji-Aware Sarcasm Detection

This project studies how emojis influence sarcasm detection in short social media texts.

For example:

- ‚ÄúGreat job üôÇ‚Äù ‚Üí genuine praise  
- ‚ÄúGreat job üôÑ‚Äù ‚Üí sarcastic criticism  

This project investigates whether retaining emoji information in the input improves sarcasm detection compared to text-only representations. 
We evaluate multiple input variants and compare strong transformer baselines with our proposed gated fusion model.

We treat this as a **binary classification task**:

- `0` ‚Üí Not Sarcastic  
- `1` ‚Üí Sarcastic  

We compare strong baselines with a proposed **emoji-aware gated fusion model built on RoBERTa**.

---

# Team Responsibilities

## Riya
- Designed and implemented the complete data pipeline
- Built emoji-aware preprocessing module
  - text-only (emojis removed)
  - full (text + emoji)
  - demojized (emoji ‚Üí text tokens)
  - emoji-only stream
- Implemented balanced train / validation split
- Ensured reproducibility using fixed random seed
- Saved split indices in splits.json
- Generated dataset statistics (class balance, emoji usage)
- Built reusable data loaders for training and evaluation
- Ensured official test set remains untouched
- Structured data module for seamless integration with models


## Aswathy
- Trained all models:
  - TF-IDF + Logistic Regression  
  - RoBERTa baselines  
  - Gated Fusion model  
- Conducted full training experiments  
- Compared variants (text-only, full, demojized)  
- Evaluated results on:
  - Full development set  
  - Emoji-only subset  
- Performed training analysis and model comparison  

## Pranjaly
- Integrated the training pipeline  
- Implemented testing scripts  
- Conducted ablation experiments  
- Performed detailed error analysis  

---

# Project Structure

```
configs/
data/
results/
src/
  ‚îú‚îÄ‚îÄ data/
  ‚îú‚îÄ‚îÄ models/
  ‚îú‚îÄ‚îÄ train/
  ‚îú‚îÄ‚îÄ eval/
README.md
```

All model outputs are saved inside:

```
results/<run_name>/
```

Each run folder contains:

- `metrics.json`  
- `confusion.json`  
- `preds_dev.csv`  
- `preds_test.csv`  
- `run_config.json`  
- `best.pt`  

---

# Setup and Installation

## 1. Clone the repository

```bash
git clone <repo_url>
cd emoji-aware-sarcasm-detection
```

## 2. Install dependencies

```bash
pip install -r requirements.txt
```

Main libraries used:

- torch  
- transformers  
- scikit-learn  
- pandas  
- numpy  

---

# How to Run the Project

## Step 1 ‚Äî Generate Train / Validation Splits

Before training any models, generate the reproducible train/dev split:

```bash
python data/prepare_splits.py

---

## Step 2 ‚Äî Run TF-IDF + Logistic Regression Baselines

```bash
python -m src.baselines.train_tfidf_lr --variant text --run_name lr_text
python -m src.baselines.train_tfidf_lr --variant demojized --run_name lr_demojized
```

---

## Step 3 ‚Äî Run RoBERTa Baselines

We evaluate three input cases:

- text-only  
- full (text + emoji)  
- demojized  

```bash
python -m src.transformers.train_transformer --config configs/transformer_text_only.json
python -m src.transformers.train_transformer --config configs/transformer_full.json
python -m src.transformers.train_transformer --config configs/transformer_demojized.json
```

---

## Step 4 ‚Äî Run Gated Fusion Model

This is our main proposed model.

```bash
python -m src.train.train_gated_fusion
```

This script:

- Trains the model  
- Saves the best checkpoint  
- Saves development predictions  
- Stores training metrics  

---

## Step 5 ‚Äî Testing

```bash
python -m src.eval.test_gated_fusion
```

After training, this:

- Loads the best checkpoint  
- Evaluates on the test set  
- Saves:
  - `test_metrics.json`
  - `preds_test.csv`

---

# Models and Variants Evaluated

We tested the following input variants:

1. Text-only  
2. Full (Text + Emoji)  
3. Demojized (Emoji ‚Üí Text description)  

Each model was evaluated on:

- Full development set  
- Emoji-only subset  

---

# Evaluation Metrics

We report:

- Accuracy  
- Macro-F1 (primary metric)  
- Confusion matrix  
- Per-class precision / recall / F1  

Macro-F1 is used because the dataset is imbalanced.

---

# Key Observations

- Emojis influence sarcasm detection in specific contexts.  
- Full RoBERTa input is a strong baseline.  
- Gated fusion helps interpret emoji-heavy tweets.  
- Performance difference is clearer on emoji-only subset.  

---

# Final Note

This project was developed for a Natural Language Understanding course.  
It explores sarcasm as a form of pragmatic meaning and studies how emoji-aware modeling improves implicit meaning understanding.
